{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/whisper.git (from -r requirements.txt (line 2))\n",
      "  Cloning https://github.com/openai/whisper.git to c:\\users\\fatoni murfid s\\appdata\\local\\temp\\pip-req-build-qhfczglf\n",
      "  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: Flask in d:\\anaconda\\envs\\ds\\lib\\site-packages (from -r requirements.txt (line 1)) (3.0.3)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in d:\\anaconda\\envs\\ds\\lib\\site-packages (from Flask->-r requirements.txt (line 1)) (3.0.3)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in d:\\anaconda\\envs\\ds\\lib\\site-packages (from Flask->-r requirements.txt (line 1)) (3.1.3)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in d:\\anaconda\\envs\\ds\\lib\\site-packages (from Flask->-r requirements.txt (line 1)) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.3 in d:\\anaconda\\envs\\ds\\lib\\site-packages (from Flask->-r requirements.txt (line 1)) (8.1.7)\n",
      "Requirement already satisfied: blinker>=1.6.2 in d:\\anaconda\\envs\\ds\\lib\\site-packages (from Flask->-r requirements.txt (line 1)) (1.8.2)\n",
      "Requirement already satisfied: numba in d:\\anaconda\\envs\\ds\\lib\\site-packages (from openai-whisper==20231117->-r requirements.txt (line 2)) (0.59.1)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\envs\\ds\\lib\\site-packages (from openai-whisper==20231117->-r requirements.txt (line 2)) (1.24.3)\n",
      "Requirement already satisfied: torch in d:\\anaconda\\envs\\ds\\lib\\site-packages (from openai-whisper==20231117->-r requirements.txt (line 2)) (2.3.0)\n",
      "Requirement already satisfied: tqdm in d:\\anaconda\\envs\\ds\\lib\\site-packages (from openai-whisper==20231117->-r requirements.txt (line 2)) (4.66.2)\n",
      "Collecting more-itertools (from openai-whisper==20231117->-r requirements.txt (line 2))\n",
      "  Downloading more_itertools-10.3.0-py3-none-any.whl.metadata (36 kB)\n",
      "Collecting tiktoken (from openai-whisper==20231117->-r requirements.txt (line 2))\n",
      "  Downloading tiktoken-0.7.0-cp311-cp311-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\envs\\ds\\lib\\site-packages (from click>=8.1.3->Flask->-r requirements.txt (line 1)) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anaconda\\envs\\ds\\lib\\site-packages (from Jinja2>=3.1.2->Flask->-r requirements.txt (line 1)) (2.1.3)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in d:\\anaconda\\envs\\ds\\lib\\site-packages (from numba->openai-whisper==20231117->-r requirements.txt (line 2)) (0.42.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in d:\\anaconda\\envs\\ds\\lib\\site-packages (from tiktoken->openai-whisper==20231117->-r requirements.txt (line 2)) (2023.10.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in d:\\anaconda\\envs\\ds\\lib\\site-packages (from tiktoken->openai-whisper==20231117->-r requirements.txt (line 2)) (2.31.0)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\envs\\ds\\lib\\site-packages (from torch->openai-whisper==20231117->-r requirements.txt (line 2)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\anaconda\\envs\\ds\\lib\\site-packages (from torch->openai-whisper==20231117->-r requirements.txt (line 2)) (4.11.0)\n",
      "Requirement already satisfied: sympy in d:\\anaconda\\envs\\ds\\lib\\site-packages (from torch->openai-whisper==20231117->-r requirements.txt (line 2)) (1.12)\n",
      "Requirement already satisfied: networkx in d:\\anaconda\\envs\\ds\\lib\\site-packages (from torch->openai-whisper==20231117->-r requirements.txt (line 2)) (3.1)\n",
      "Requirement already satisfied: fsspec in d:\\anaconda\\envs\\ds\\lib\\site-packages (from torch->openai-whisper==20231117->-r requirements.txt (line 2)) (2024.3.1)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in d:\\anaconda\\envs\\ds\\lib\\site-packages (from torch->openai-whisper==20231117->-r requirements.txt (line 2)) (2021.4.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in d:\\anaconda\\envs\\ds\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->openai-whisper==20231117->-r requirements.txt (line 2)) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in d:\\anaconda\\envs\\ds\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->openai-whisper==20231117->-r requirements.txt (line 2)) (2021.12.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\envs\\ds\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117->-r requirements.txt (line 2)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\envs\\ds\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117->-r requirements.txt (line 2)) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\envs\\ds\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117->-r requirements.txt (line 2)) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\envs\\ds\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117->-r requirements.txt (line 2)) (2024.7.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\anaconda\\envs\\ds\\lib\\site-packages (from sympy->torch->openai-whisper==20231117->-r requirements.txt (line 2)) (1.3.0)\n",
      "Downloading more_itertools-10.3.0-py3-none-any.whl (59 kB)\n",
      "   ---------------------------------------- 0.0/59.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 59.2/59.2 kB 1.6 MB/s eta 0:00:00\n",
      "Downloading tiktoken-0.7.0-cp311-cp311-win_amd64.whl (799 kB)\n",
      "   ---------------------------------------- 0.0/799.0 kB ? eta -:--:--\n",
      "    -------------------------------------- 20.5/799.0 kB 217.9 kB/s eta 0:00:04\n",
      "   -- ------------------------------------ 61.4/799.0 kB 648.1 kB/s eta 0:00:02\n",
      "   -- ------------------------------------ 61.4/799.0 kB 648.1 kB/s eta 0:00:02\n",
      "   -- ------------------------------------ 61.4/799.0 kB 648.1 kB/s eta 0:00:02\n",
      "   ---- ---------------------------------- 92.2/799.0 kB 348.6 kB/s eta 0:00:03\n",
      "   ---- ---------------------------------- 92.2/799.0 kB 348.6 kB/s eta 0:00:03\n",
      "   ----- -------------------------------- 112.6/799.0 kB 344.8 kB/s eta 0:00:02\n",
      "   ------ ------------------------------- 143.4/799.0 kB 369.8 kB/s eta 0:00:02\n",
      "   -------- ----------------------------- 174.1/799.0 kB 402.6 kB/s eta 0:00:02\n",
      "   --------- ---------------------------- 194.6/799.0 kB 420.8 kB/s eta 0:00:02\n",
      "   --------- ---------------------------- 204.8/799.0 kB 414.8 kB/s eta 0:00:02\n",
      "   ----------- -------------------------- 245.8/799.0 kB 430.6 kB/s eta 0:00:02\n",
      "   ------------- ------------------------ 276.5/799.0 kB 448.2 kB/s eta 0:00:02\n",
      "   ------------- ------------------------ 286.7/799.0 kB 442.4 kB/s eta 0:00:02\n",
      "   --------------- ---------------------- 327.7/799.0 kB 461.4 kB/s eta 0:00:02\n",
      "   ----------------- -------------------- 358.4/799.0 kB 473.7 kB/s eta 0:00:01\n",
      "   ------------------- ------------------ 419.8/799.0 kB 513.9 kB/s eta 0:00:01\n",
      "   --------------------- ---------------- 450.6/799.0 kB 531.1 kB/s eta 0:00:01\n",
      "   ----------------------- -------------- 501.8/799.0 kB 561.7 kB/s eta 0:00:01\n",
      "   ------------------------- ------------ 532.5/799.0 kB 575.8 kB/s eta 0:00:01\n",
      "   --------------------------- ---------- 583.7/799.0 kB 591.8 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 614.4/799.0 kB 604.2 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 655.4/799.0 kB 616.2 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 665.6/799.0 kB 599.0 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 696.3/799.0 kB 601.3 kB/s eta 0:00:01\n",
      "   ---------------------------------- --- 716.8/799.0 kB 594.7 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 747.5/799.0 kB 589.8 kB/s eta 0:00:01\n",
      "   -------------------------------------  778.2/799.0 kB 599.1 kB/s eta 0:00:01\n",
      "   -------------------------------------- 799.0/799.0 kB 600.7 kB/s eta 0:00:00\n",
      "Building wheels for collected packages: openai-whisper\n",
      "  Building wheel for openai-whisper (pyproject.toml): started\n",
      "  Building wheel for openai-whisper (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=812160 sha256=8ca25f54235ba2e9501c4b8b1b6263eafda05357b3bf50da7889db33efbb0f6d\n",
      "  Stored in directory: C:\\Users\\Fatoni Murfid S\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-7rmkpyv8\\wheels\\1f\\1d\\98\\9583695e6695a6ac0ad42d87511097dce5ba486647dbfecb0e\n",
      "Successfully built openai-whisper\n",
      "Installing collected packages: more-itertools, tiktoken, openai-whisper\n",
      "Successfully installed more-itertools-10.3.0 openai-whisper-20231117 tiktoken-0.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git 'C:\\Users\\Fatoni Murfid S\\AppData\\Local\\Temp\\pip-req-build-qhfczglf'\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"filename\": \"southpark-the-coon.mp3\",\n",
      "  \"stats\": {\n",
      "    \"file_size_in_bytes\": 1393951,\n",
      "    \"total_processing_time\": 12.167830228805542,\n",
      "    \"words_per_second\": 66.73\n",
      "  },\n",
      "  \"transcription\": \" The city isn't what it used to be. It all happened so fast. Everything went to crap. It's like everyone's sense of morals just disappeared. The bad economy made things worse. The jobs started drying up. Then the stores had to shut down. Then a black man was elected president. He was supposed to change things. He didn't. As more and more people turned to crime and violence, the town becomes gripped in fear. Doc Tams, the city needs protection. There is an animal that lives by night. Searches through trash cans and cleans out the garbage. To clean the trash can of society I've chosen to become more than a man. I am the hero this town needs. I am the king. I am the king. I am the king. I am the king. I am the king. I am the king. I am the king. I am the king. I am the king. I am the king. I am the king.\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = 'http://localhost:5000/transcribe'\n",
    "file_path = 'southpark-the-coon.mp3'\n",
    "\n",
    "# Buka file audio dan kirimkan sebagai bagian dari form-data\n",
    "with open(file_path, 'rb') as f:\n",
    "    files = {'file': f}\n",
    "    response = requests.post(url, files=files)\n",
    "\n",
    "# Tampilkan respons dari server\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in d:\\anaconda\\envs\\ds\\lib\\site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00jhJ_YAsU90J3hpG-vNI1I9QBm_Voefj8NMcR-OEY8\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filename': '02 At The Campus.mp3', 'stats': {'file_size_in_bytes': 1418362, 'total_processing_time': 60.357834815979004, 'words_per_second': 7.62}, 'transcription': ' Pelajaran 2 di kampus. Sari lahir di Jakarta. Keluarga Sari tinggal di pasar minggu. Sari pergi ke kampus Universitas Indonesia di Depok. Dia ikut ayahnya. Ayah Sari punya mobil. Di kampus Sari belajar hubungan internasional. Sore dia pulang dari kampus. Di kantin Sari minum Pepsi. Di rumah dia minum teh. Adeknya suka minum es jeruk. Kakaknya suka minum bir bintang. Mereka suka makan nasi goreng. Sari makan siang di kampus. Di kampus dia makan mie goreng.'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = 'http://localhost:5000/transcribe'\n",
    "files = {'file': open('sounds/02 At The Campus.mp3', 'rb')}\n",
    "headers = {'x-api-key': '00jhJ_YAsU90J3hpG-vNI1I9QBm_Voefj8NMcR-OEY8'}\n",
    "\n",
    "response = requests.post(url, files=files, headers=headers)\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response time for GPU_FAST-API: 74.96329426765442 seconds\n",
      "Response time for GPU_FLASK: 74.77001285552979 seconds\n",
      "Response from URL1: {'transcription': ' Pelajaran 2 di kampus. Sari lahir di Jakarta. Keluarga Sari tinggal di pasar minggu. Sari pergi ke kampus Universitas Indonesia di Depok. Dia ikut ayahnya. Ayah Sari punya mobil. Di kampus Sari belajar hubungan internasional. Sore dia pulang dari kampus. Di kantin Sari minum Pepsi. Di rumah dia minum teh. Adeknya suka minum es jeruk. Kakaknya suka minum bir bintang. Mereka suka makan nasi goreng. Sari makan siang di kampus. Di kampus dia makan mie goreng.', 'stats': {'total_processing_time': 72.86748790740967, 'words_per_second': 6.31, 'file_size_in_bytes': 1418362}, 'filename': '02 At The Campus.mp3'}\n",
      "Response from URL2: {'filename': '02 At The Campus.mp3', 'stats': {'file_size_in_bytes': 1418362, 'total_processing_time': 72.6709976196289, 'words_per_second': 6.33}, 'transcription': ' Pelajaran 2 di kampus. Sari lahir di Jakarta. Keluarga Sari tinggal di pasar minggu. Sari pergi ke kampus Universitas Indonesia di Depok. Dia ikut ayahnya. Ayah Sari punya mobil. Di kampus Sari belajar hubungan internasional. Sore dia pulang dari kampus. Di kantin Sari minum Pepsi. Di rumah dia minum teh. Adeknya suka minum es jeruk. Kakaknya suka minum bir bintang. Mereka suka makan nasi goreng. Sari makan siang di kampus. Di kampus dia makan mie goreng.'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "url1 = 'http://localhost:8000/transcribe'\n",
    "url2 = 'http://localhost:5000/transcribe'\n",
    "files = {'file': open('sounds/02 At The Campus.mp3', 'rb')}\n",
    "headers = {'x-api-key': '00jhJ_YAsU90J3hpG-vNI1I9QBm_Voefj8NMcR-OEY8'}\n",
    "\n",
    "# Measure response time for the first URL\n",
    "start_time_1 = time.time()\n",
    "response1 = requests.post(url1, files=files, headers=headers)\n",
    "end_time_1 = time.time()\n",
    "response_time_1 = end_time_1 - start_time_1\n",
    "\n",
    "files['file'].seek(0)\n",
    "start_time_2 = time.time()\n",
    "response2 = requests.post(url2, files=files, headers=headers)\n",
    "end_time_2 = time.time()\n",
    "response_time_2 = end_time_2 - start_time_2\n",
    "\n",
    "print(f\"Response time for GPU_FAST-API: {response_time_1} seconds\")\n",
    "print(f\"Response time for GPU_FLASK: {response_time_2} seconds\")\n",
    "\n",
    "print(\"Response from URL1:\", response1.json())\n",
    "print(\"Response from URL2:\", response2.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.4.0\n",
      "CUDA available: True\n",
      "Number of GPUs: 1\n",
      "Current GPU: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)    \n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "    print(\"Current GPU:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "else:\n",
    "    print(\"CUDA is not available. PyTorch cannot use the GPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)  # Check PyTorch version\n",
    "print(torch.version.cuda)  # Check CUDA version PyTorch is built with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Number of GPUs: 1\n",
      "Current GPU: b'NVIDIA GeForce RTX 3050 Laptop GPU'\n"
     ]
    }
   ],
   "source": [
    "from numba import cuda\n",
    "\n",
    "print(\"CUDA available:\", cuda.is_available())\n",
    "\n",
    "if cuda.is_available():\n",
    "    print(\"Number of GPUs:\", len(cuda.list_devices()))\n",
    "    print(\"Current GPU:\", cuda.get_current_device().name)\n",
    "else:\n",
    "    print(\"CUDA is not available. Numba cannot use the GPU.\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPU:  1\n",
      "GPU Name:  NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Number of GPU: \", torch.cuda.device_count())\n",
    "print(\"GPU Name: \", torch.cuda.get_device_name())\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
